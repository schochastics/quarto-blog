{
  "hash": "609eb70ddcd00ec00c2fc57f8d3356b6",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: One billion row challenge using base R\nauthor:\n- name: David Schoch\n  orcid: 0000-0003-2952-4812\ndate: '2024-01-08'\ncategories: R\n\n---\n\n\n\n\n**Edit (2025-01-30): The text and benchmarks about `Rfast` are not correct anymore. I had to rerub the post for various reasons and the code didnt work anymore. After fixing it, Rfast was running slower than back in January 2024. What du we learn from this? Use renv....** \n\n*One of my new years resolutions is to blog a bit more on the random shenanigans I do with R. This is one of those.*[^1]\n\nThe [One Billion Row challenge](https://www.morling.dev/blog/one-billion-row-challenge/) by Gunnar Morling is as follows:\n\n> write a Java program for retrieving temperature measurement values from a text file and calculating the min, mean, and max temperature per weather station. Thereâ€™s just one caveat: the file has 1,000,000,000 rows!\n\nI didn't take long, also thanks to [Hacker News](https://news.ycombinator.com/item?id=38851337), that the challenge spread to other programming languages. The original repository contains a [show & tell](https://github.com/gunnarmorling/1brc/discussions/categories/show-and-tell) where other results can be discussed.\n\nObviously it also spread to R and there is a [GitHub repository](https://github.com/alejandrohagan/1br) from Alejandro Hagan dedicated to the challenge. There were some [critical discussions](https://github.com/alejandrohagan/1br/issues/5) on the seemingly bad performance of `data.table` but that issue thread also evolved to a discussion on other solutions. \n\nThe obvious candidates for fast solutions with R are [`dplyr`](https://github.com/tidyverse/dplyr), [`data.table`](https://github.com/Rdatatable/data.table), [`collapse`](https://github.com/SebKrantz/collapse), and [`polars`](https://github.com/pola-rs/r-polars). From those, it appears that polars might solve the tasks the [fastest](https://github.com/alejandrohagan/1br/issues/5#issuecomment-1879737918).\n\nI was curious, how far one can get with base R. \n\n## Creating the data\n\nThe R repository contains a [script](https://github.com/alejandrohagan/1br/blob/main/generate_data.R) to generate benchmark data. For the purpose of this post, I created files with 1e6 and 1e8 rows. Unfortunately, my personal laptop cannot handle 1 billion rows without dying.\n\n## Reading the data\n\nAll base R functions will profit from reading the state column as a factor instead of a usual string. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nD <- data.table::fread(\"measurements1e6.csv\", stringsAsFactors = TRUE)\nD\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         measurement  state\n               <num> <fctr>\n      1:   0.9819694     NC\n      2:   0.4687150     MA\n      3:  -0.1079713     TX\n      4:  -0.2128782     VT\n      5:   1.1580985     OR\n     ---                   \n 999996:   0.7432489     FL\n 999997:  -1.6855612     KS\n 999998:  -0.1184549     TX\n 999999:   1.2774375     MS\n1000000:  -0.2800853     MD\n```\n\n\n:::\n:::\n\n\n\n\nWho would have thought that `stringAsFactors = TRUE` can be useful.\n\n## The obvious: aggregate and split/lapply\n\nThe most obvious choice for me was to use `aggregate()`.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum_stats_vec <- function(x) c(min = min(x), max = max(x), mean = mean(x))\naggregate(measurement ~ state, data = D, FUN = sum_stats_vec) |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  state measurement.min measurement.max measurement.mean\n1    AK   -4.1044940643    4.2710094897     0.0030819995\n2    AL   -3.6350249689    4.5386719185    -0.0078110501\n3    AR   -3.8138004849    4.1011149408     0.0084538758\n4    AZ   -4.4150509298    3.9651248287     0.0002287343\n5    CA   -4.1597256267    4.1024463673     0.0136493032\n6    CO   -3.8604891180    4.2314151167    -0.0013340964\n```\n\n\n:::\n:::\n\n\n\n\nI was pretty sure that this might be the best solution.\n\nThe other obvious solution is to split the data frame according to stats and then `lapply` the stats calculation on each list element.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsplit_lapply <- function(D) {\n  result <- lapply(split(D, D$state), function(x) {\n    stats <- sum_stats_vec(x$measurement)\n    data.frame(\n      state = unique(x$state),\n      min = stats[1],\n      max = stats[2],\n      mean = stats[3]\n    )\n  })\n  do.call(\"rbind\", result)\n}\nsplit_lapply(D) |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   state       min      max          mean\nAK    AK -4.104494 4.271009  0.0030819995\nAL    AL -3.635025 4.538672 -0.0078110501\nAR    AR -3.813800 4.101115  0.0084538758\nAZ    AZ -4.415051 3.965125  0.0002287343\nCA    CA -4.159726 4.102446  0.0136493032\nCO    CO -3.860489 4.231415 -0.0013340964\n```\n\n\n:::\n:::\n\n\n\n\n## The elegant: by\n\nI stumbled upon `by` when searching for alternatives. I think it is a quite elegant way of solving a group/summarize task with base R. Unfortunately it returns a list and not a data frame or matrix (I made that an implicit requirement).  \n\nIn the help for `by` I stumbled upon a function I wasn't aware of yet: `array2DF`! \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\narray2DF(by(D$measurement, D$state, sum_stats_vec)) |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  D$state       min      max          mean\n1      AK -4.104494 4.271009  0.0030819995\n2      AL -3.635025 4.538672 -0.0078110501\n3      AR -3.813800 4.101115  0.0084538758\n4      AZ -4.415051 3.965125  0.0002287343\n5      CA -4.159726 4.102446  0.0136493032\n6      CO -3.860489 4.231415 -0.0013340964\n```\n\n\n:::\n:::\n\n\n\n\nDoes exactly what is needed here. For the benchmarks, I will also include a version without the `array2DF` call, to check its overhead.\n\n## Another apply: tapply\n\nIn the help for `by`, I also stumbled upon this sentence \n\n> Function `by` is an object-oriented wrapper for `tapply` applied to data frames.\n\nSo maybe we can construct a solution that uses tapply, but without any inbuilt overhead in `by`.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndo.call(\"rbind\", tapply(D$measurement, D$state, sum_stats_vec)) |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         min      max          mean\nAK -4.104494 4.271009  0.0030819995\nAL -3.635025 4.538672 -0.0078110501\nAR -3.813800 4.101115  0.0084538758\nAZ -4.415051 3.965125  0.0002287343\nCA -4.159726 4.102446  0.0136493032\nCO -3.860489 4.231415 -0.0013340964\n```\n\n\n:::\n:::\n\n\n\n\nAt this point, I was also curious if the `do.call(\"rbind\",list)` can be sped up, so I constructed a second tapply solution.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsapply(tapply(D$measurement, D$state, sum_stats_vec), rbind) |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            AK          AL           AR            AZ         CA           CO\n[1,] -4.104494 -3.63502497 -3.813800485 -4.4150509298 -4.1597256 -3.860489118\n[2,]  4.271009  4.53867192  4.101114941  3.9651248287  4.1024464  4.231415117\n[3,]  0.003082 -0.00781105  0.008453876  0.0002287343  0.0136493 -0.001334096\n               CT           DE           FL           GA           HI\n[1,] -3.912464242 -4.139947796 -3.741261373 -3.946609985 -4.012318121\n[2,]  4.424393025  4.103203429  3.614619126  3.928906751  3.542107460\n[3,]  0.003656912  0.003007263  0.003644747  0.009984163  0.008314731\n              IA           ID           IL          IN          KS           KY\n[1,] -3.79082275 -3.964539123 -3.894335277 -4.03965959 -4.02908514 -4.048575194\n[2,]  4.16662411  3.577876528  3.795398515  3.89974329  3.62109161  3.867433732\n[3,]  0.01217377  0.003644028  0.002829236 -0.01164553  0.01471751 -0.001413558\n               LA           MA          MD           ME           MI\n[1,] -3.839122088 -4.037138674 -3.74470558 -3.923094861 -3.952862515\n[2,]  3.569243248  3.792630597  3.71628977  3.663992861  4.155783544\n[3,]  0.001506674 -0.006959067 -0.01049735  0.001780304  0.005783873\n               MN            MO           MS           MT           NC\n[1,] -4.410583765 -3.9391034949 -4.110337731 -4.087100626 -4.377367847\n[2,]  4.311070983  4.4288231699  4.040434363  4.162059065  3.981134558\n[3,]  0.002225353 -0.0005302625  0.004016168 -0.005199102 -0.007913031\n               ND           NE           NH           NJ           NM\n[1,] -4.068938069 -4.060063227 -3.915743483 -3.897490988 -3.807738095\n[2,]  3.743970751  4.178671897  4.127142052  4.092974297  3.853332924\n[3,] -0.006566703 -0.001995199  0.001732865 -0.008216333 -0.004512297\n               NV           NY           OH          OK            OR\n[1,] -4.499979315 -4.106887785 -3.970732385 -4.01749904 -3.7554051729\n[2,]  3.984158422  3.775390296  4.055413781  3.92196743  4.2991202553\n[3,]  0.005909496 -0.004012489  0.003917906  0.00272036  0.0004278573\n              PA           RI          SC           SD           TN\n[1,] -3.87864920 -3.656146720 -3.63600174 -4.252121844 -3.630113181\n[2,]  4.18986336  4.257514030  4.11314448  3.742961731  3.920525368\n[3,] -0.00303136 -0.004191743 -0.01102259  0.007743447  0.006712156\n               TX           UT          VA            VT           WA\n[1,] -4.205888754 -4.034818321 -4.29800813 -3.7493697276 -3.760239357\n[2,]  3.929358388  3.995302051  3.86146553  3.9909618948  3.874636930\n[3,] -0.007846478 -0.005618143 -0.01242856 -0.0005795627  0.006844534\n               WI         WV           WY\n[1,] -3.763026455 -3.6034317 -3.860997763\n[2,]  4.322641193  3.9029500  4.116532763\n[3,]  0.002972319 -0.0090395 -0.002380265\n```\n\n\n:::\n:::\n\n\n\n\nand we should obviously also include our new found `array2DF`\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\narray2DF(tapply(D$measurement, D$state, sum_stats_vec)) |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Var1       min      max          mean\n1   AK -4.104494 4.271009  0.0030819995\n2   AL -3.635025 4.538672 -0.0078110501\n3   AR -3.813800 4.101115  0.0084538758\n4   AZ -4.415051 3.965125  0.0002287343\n5   CA -4.159726 4.102446  0.0136493032\n6   CO -3.860489 4.231415 -0.0013340964\n```\n\n\n:::\n:::\n\n\n\n\n## The obscure: reduce\n\nI thought that this should be it, but then I remembered `reduce` exists.\nThe solution is somewhat similar to split/lapply.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreduce <- function(D) {\n  state_list <- split(D$measurement, D$state)\n  Reduce(function(x, y) {\n    res <- sum_stats_vec(state_list[[y]])\n    rbind(x, data.frame(state = y, mean = res[1], min = res[2], max = res[3]))\n  }, names(state_list), init = NULL)\n}\n\nreduce(D) |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     state      mean      min           max\nmin     AK -4.104494 4.271009  0.0030819995\nmin1    AL -3.635025 4.538672 -0.0078110501\nmin2    AR -3.813800 4.101115  0.0084538758\nmin3    AZ -4.415051 3.965125  0.0002287343\nmin4    CA -4.159726 4.102446  0.0136493032\nmin5    CO -3.860489 4.231415 -0.0013340964\n```\n\n\n:::\n:::\n\n\n\n\n## The unfair contender: Rfast\n\nPondering about how this functions could be sped up in general, I remembered the package [`Rfast`](https://github.com/RfastOfficial/Rfast/) and managed to construct a solution using this package.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nRfast <- function(D) {\n  lev_int <- as.numeric(D$state)\n  minmax <- Rfast::group(D$measurement, ina = lev_int, method = \"min\")\n  data.frame(\n    state = levels(D$state),\n    mean = Rfast::group(D$measurement, lev_int, method = \"mean\"),\n    min = Rfast::group(D$measurement, ina = lev_int, method = \"min\"),\n    max = Rfast::group(D$measurement, ina = lev_int, method = \"max\")\n  )\n}\n\nRfast(D) |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  state          mean       min      max\n1    AK  0.0030819995 -4.104494 4.271009\n2    AL -0.0078110501 -3.635025 4.538672\n3    AR  0.0084538758 -3.813800 4.101115\n4    AZ  0.0002287343 -4.415051 3.965125\n5    CA  0.0136493032 -4.159726 4.102446\n6    CO -0.0013340964 -3.860489 4.231415\n```\n\n\n:::\n:::\n\n\n\n\nPretty sure that this will be the fastest, maybe even competitive with the other big packages!\n\n## Benchmark\n\nFor better readability I reorder the benchmark results from `microbenchmark` according to median runtime, with a function provided by [Dirk Eddelbuettel](https://github.com/eddelbuettel/dang/blob/master/R/reorderMicrobenchmarkResults.R).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreorderMicrobenchmarkResults <- function(res, order = \"median\") {\n  stopifnot(\"Argument 'res' must be a 'microbenchmark' result\" = inherits(res, \"microbenchmark\"))\n\n  smry <- summary(res)\n  res$expr <- factor(res$expr,\n    levels = levels(res$expr)[order(smry[[\"median\"]])],\n    ordered = TRUE\n  )\n  res\n}\n```\n:::\n\n\n\n\nFirst up the \"small\" dataset with 1e6 rows. I added the `dplyr` and `data.table` results as references.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum_stats_list <- function(x) list(min = min(x), max = max(x), mean = mean(x))\nsum_stats_tibble <- function(x) tibble::tibble(min = min(x), max = max(x), mean = mean(x))\n\nbench1e6 <- microbenchmark::microbenchmark(\n  aggregate = aggregate(measurement ~ state, data = D, FUN = sum_stats_vec),\n  split_lapply = split_lapply(D),\n  array2DF_by = array2DF(by(D$measurement, D$state, sum_stats_vec)),\n  raw_by = by(D$measurement, D$state, sum_stats_vec),\n  docall_tapply = do.call(\"rbind\", tapply(D$measurement, D$state, sum_stats_vec)),\n  sapply_tapply = sapply(tapply(D$measurement, D$state, sum_stats_vec), rbind),\n  array2DF_tapply = array2DF(tapply(D$measurement, D$state, sum_stats_vec)),\n  reduce = reduce(D),\n  Rfast = Rfast(D),\n  dplyr = D |> dplyr::group_by(state) |> dplyr::summarise(sum_stats_tibble(measurement)) |> dplyr::ungroup(),\n  datatable = D[, .(sum_stats_list(measurement)), by = state],\n  times = 25\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in microbenchmark::microbenchmark(aggregate = aggregate(measurement ~ :\nless accurate nanosecond times to avoid potential integer overflows\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot2::autoplot(reorderMicrobenchmarkResults(bench1e6))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot-bench1e6-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|expr            |        min|         lq|       mean|     median|         uq|       max| neval|\n|:---------------|----------:|----------:|----------:|----------:|----------:|---------:|-----:|\n|datatable       |   6.958848|   7.405297|   8.297311|   7.933295|   8.546245|  13.83311|    25|\n|Rfast           |  11.743384|  12.139526|  13.755574|  12.197664|  12.969981|  41.29192|    25|\n|sapply_tapply   |  12.767072|  13.401711|  14.626178|  13.467024|  13.522784|  41.15502|    25|\n|docall_tapply   |  12.923692|  13.404417|  13.476324|  13.472600|  13.535248|  15.58541|    25|\n|array2DF_tapply |  12.840462|  13.439144|  17.402112|  13.643693|  13.766037|  45.40615|    25|\n|raw_by          |  17.751565|  18.374970|  22.616228|  18.889930|  20.492866|  48.64662|    25|\n|array2DF_by     |  17.893876|  18.837942|  21.434011|  18.973857|  21.597898|  48.19763|    25|\n|reduce          |  19.132240|  20.191311|  22.698038|  20.466585|  21.401713|  49.20381|    25|\n|dplyr           |  18.921623|  20.267776|  24.996619|  20.922218|  22.132005| 122.47163|    25|\n|split_lapply    |  29.365840|  30.348610|  36.139006|  32.277455|  33.338781|  62.49318|    25|\n|aggregate       | 121.751919| 126.151547| 135.702799| 129.262094| 145.984149| 157.07530|    25|\n\n\n:::\n:::\n\n\n\n\nFirst of, I was very surprised by the bad performance of `aggregate`. I looked at the source code and it appears to be a more fancy lapply/split type of functions with a lot of `if/else` and `for` which do slow down the function heavily. For the benchmark with the bigger dataset, I actually discarded the function because it was way too slow.\n\nApart from that, there are three groups. `Rfast` and `data.table` are the fastest. The second group are the `tapply` versions. I am quite pleased with the fact that the data frame building via `do.call`, `sapply` and `array2DF` are very much comparable, because I really like my `array2DF` discovery. The remaining solutions are pretty much comparable. I am surprised though, that `dplyr` falls behind many of the base solutions.[^2]\n\nMoving on to the 100 million file to see if size makes a difference.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nD <- data.table::fread(\"measurements1e8.csv\", stringsAsFactors = TRUE)\n\nbench1e8 <- microbenchmark::microbenchmark(\n  # aggregate = aggregate(measurement ~ state, data = D, FUN = sum_stats_vec),\n  split_lapply = split_lapply(D),\n  array2DF_by = array2DF(by(D$measurement, D$state, sum_stats_vec)),\n  raw_by = by(D$measurement, D$state, sum_stats_vec),\n  docall_tapply = do.call(\"rbind\", tapply(D$measurement, D$state, sum_stats_vec)),\n  sapply_tapply = sapply(tapply(D$measurement, D$state, sum_stats_vec), rbind),\n  array2DF_tapply = array2DF(tapply(D$measurement, D$state, sum_stats_vec)),\n  reduce = reduce(D),\n  Rfast = Rfast(D),\n  dplyr = D |> dplyr::group_by(state) |> dplyr::summarise(sum_stats_tibble(measurement)) |> dplyr::ungroup(),\n  datatable = D[, .(sum_stats_list(measurement)), by = state],\n  times = 10\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot2::autoplot(reorderMicrobenchmarkResults(bench1e8))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot-bench1e8-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|expr            |       min|        lq|     mean|    median|       uq|      max| neval|\n|:---------------|---------:|---------:|--------:|---------:|--------:|--------:|-----:|\n|datatable       |  952.2624|  965.2889| 1025.441|  979.1874| 1030.698| 1307.946|    10|\n|dplyr           | 1122.5819| 1148.4585| 1217.971| 1163.2574| 1257.394| 1434.936|    10|\n|Rfast           | 1238.9806| 1279.6060| 1322.938| 1300.6930| 1327.787| 1545.191|    10|\n|reduce          | 1300.1480| 1311.4900| 1355.998| 1325.8422| 1379.634| 1474.427|    10|\n|array2DF_tapply | 1330.6773| 1344.4986| 1371.125| 1353.8964| 1394.718| 1452.030|    10|\n|docall_tapply   | 1316.0870| 1341.4056| 1379.221| 1366.9425| 1382.036| 1574.872|    10|\n|sapply_tapply   | 1313.0628| 1328.9188| 1448.447| 1404.6124| 1446.478| 1742.124|    10|\n|array2DF_by     | 2182.8691| 2231.2909| 2262.856| 2245.7254| 2273.568| 2395.180|    10|\n|raw_by          | 2186.5236| 2228.7242| 2271.197| 2263.1893| 2313.219| 2334.976|    10|\n|split_lapply    | 2513.3321| 2591.7153| 2638.842| 2627.3450| 2686.659| 2792.027|    10|\n\n\n:::\n:::\n\n\n\n\nAgain we see three groups, but this time with clearer cut-offs. `Rfast` and `data.table` dominate and Rfast actually has a slight edge! The second group are `tapply`, `reduce` and `dplyr`. Surprisingly, `by` falls behind here, together with `split/lapply`.\n\n**Update**(2024-01-09)  \n\nI managed to run some of the functions on a 1e9 file.\n\n```r\nbench1e9 <- microbenchmark::microbenchmark(\n    docall_tapply = do.call(\"rbind\", tapply(D$measurement, D$state, sum_stats_vec)),\n    reduce = reduce(D),\n    Rfast = Rfast(D),\n    dplyr = D |> dplyr::group_by(state) |> dplyr::summarise(sum_stats_tibble(measurement)) |> dplyr::ungroup(),\n    datatable = D[, .(sum_stats_list(measurement)), by = state],\n    times = 5\n)\n```\n![](bench1e9.png)\n\nThe previously fastest base solutions fall of a little bit, but are in my opinion still very good and still comparable with `dplyr`! Also, I learned that one can reorder microbenchmark results with the print command!\n\n## Summary\n\nThis was a fun little exercise, and I think I learned a lot of new things about base R, especially the existence of `arry2DF`! \n\nWhat was surprising is how competitive base R actually is with the \"big guns\". I was expecting a much bigger margin between data.table and the base solutions, but that was not the case. \n\n[^1]: Also inspired by a post of [Danielle Navarro](https://blog.djnavarro.net/posts/2023-12-27_seedcatcher/) about the cultural loss of today's serious blogging business.\n\n[^2]: It is far more readable though.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}