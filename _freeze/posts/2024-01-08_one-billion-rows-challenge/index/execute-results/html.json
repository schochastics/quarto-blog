{
  "hash": "01e83a57fb1fb64ba791a9f9011cda01",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"One billion row challenge using base R\"\nauthor:\n  - name: David Schoch\n    orcid: 0000-0003-2952-4812\ndate: 2024-01-08\ncategories: [R]\n---\n\n\n*One of my new years resolutions is to blog a bit more on the random shenanigans I do with R. This is one of those.*[^1]\n\nThe [One Billion Row challenge](https://www.morling.dev/blog/one-billion-row-challenge/) by Gunnar Morling is as follows:\n\n> write a Java program for retrieving temperature measurement values from a text file and calculating the min, mean, and max temperature per weather station. Thereâ€™s just one caveat: the file has 1,000,000,000 rows!\n\nI didn't take long, also thanks to [Hacker News](https://news.ycombinator.com/item?id=38851337), that the challenge spread to other programming languages. The original repository contains a [show & tell](https://github.com/gunnarmorling/1brc/discussions/categories/show-and-tell) where other results can be discussed.\n\nObviously it also spread to R and there is a [GitHub repository](https://github.com/alejandrohagan/1br) from Alejandro Hagan dedicated to the challenge. There were some [critical discussions](https://github.com/alejandrohagan/1br/issues/5) on the seemingly bad performance of `data.table` but that issue thread also evolved to a discussion on other solutions. \n\nThe obvious candidates for fast solutions with R are [`dplyr`](https://github.com/tidyverse/dplyr), [`data.table`](https://github.com/Rdatatable/data.table), [`collapse`](https://github.com/SebKrantz/collapse), and [`polars`](https://github.com/pola-rs/r-polars). From those, it appears that polars might solve the tasks the [fastest](https://github.com/alejandrohagan/1br/issues/5#issuecomment-1879737918).\n\nI was curious, how far one can get with base R. \n\n## Creating the data\n\nThe R repository contains a [script](https://github.com/alejandrohagan/1br/blob/main/generate_data.R) to generate benchmark data. For the purpose of this post, I created files with 1e6 and 1e8 rows. Unfortunately, my personal laptop cannot handle 1 billion rows without dying.\n\n## Reading the data\n\nAll base R functions will profit from reading the state column as a factor instead of a usual string. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nD <- data.table::fread(\"measurements1e6.csv\", stringsAsFactors = TRUE)\nD\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         measurement state\n      1:    0.981969    NC\n      2:    0.468715    MA\n      3:   -0.107971    TX\n      4:   -0.212878    VT\n      5:    1.158098    OR\n     ---                  \n 999996:    0.743249    FL\n 999997:   -1.685561    KS\n 999998:   -0.118455    TX\n 999999:    1.277437    MS\n1000000:   -0.280085    MD\n```\n\n\n:::\n:::\n\n\nWho would have thought that `stringAsFactors = TRUE` can be useful.\n\n## The obvious: aggregate and split/lapply\n\nThe most obvious choice for me was to use `aggregate()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum_stats_vec <- function(x) c(min = min(x), max = max(x), mean = mean(x))\naggregate(measurement ~ state, data = D, FUN = sum_stats_vec) |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  state measurement.min measurement.max measurement.mean\n1    AK    -4.104494064     4.271009490      0.003082000\n2    AL    -3.635024969     4.538671919     -0.007811050\n3    AR    -3.813800485     4.101114941      0.008453876\n4    AZ    -4.415050930     3.965124829      0.000228734\n5    CA    -4.159725627     4.102446367      0.013649303\n6    CO    -3.860489118     4.231415117     -0.001334096\n```\n\n\n:::\n:::\n\n\nI was pretty sure that this might be the best solution.\n\nThe other obvious solution is to split the data frame according to stats and then `lapply` the stats calculation on each list element.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsplit_lapply <- function(D) {\n    result <- lapply(split(D, D$state), function(x) {\n        stats <- sum_stats_vec(x$measurement)\n        data.frame(\n            state = unique(x$state),\n            min = stats[1],\n            max = stats[2],\n            mean = stats[3]\n        )\n    })\n    do.call(\"rbind\", result)\n}\nsplit_lapply(D) |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   state      min     max         mean\nAK    AK -4.10449 4.27101  0.003082000\nAL    AL -3.63502 4.53867 -0.007811050\nAR    AR -3.81380 4.10111  0.008453876\nAZ    AZ -4.41505 3.96512  0.000228734\nCA    CA -4.15973 4.10245  0.013649303\nCO    CO -3.86049 4.23142 -0.001334096\n```\n\n\n:::\n:::\n\n\n## The elegant: by\n\nI stumbled upon `by` when searching for alternatives. I think it is a quite elegant way of solving a group/summarize task with base R. Unfortunately it returns a list and not a data frame or matrix (I made that an implicit requirement).  \n\nIn the help for `by` I stumbled upon a function I wasn't aware of yet: `array2DF`! \n\n::: {.cell}\n\n```{.r .cell-code}\narray2DF(by(D$measurement, D$state, sum_stats_vec)) |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  D$state      min     max         mean\n1      AK -4.10449 4.27101  0.003082000\n2      AL -3.63502 4.53867 -0.007811050\n3      AR -3.81380 4.10111  0.008453876\n4      AZ -4.41505 3.96512  0.000228734\n5      CA -4.15973 4.10245  0.013649303\n6      CO -3.86049 4.23142 -0.001334096\n```\n\n\n:::\n:::\n\n\nDoes exactly what is needed here. For the benchmarks, I will also include a version without the `array2DF` call, to check its overhead.\n\n## Another apply: tapply\n\nIn the help for `by`, I also stumbled upon this sentence \n\n> Function `by` is an object-oriented wrapper for `tapply` applied to data frames.\n\nSo maybe we can construct a solution that uses tapply, but without any inbuilt overhead in `by`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndo.call(\"rbind\", tapply(D$measurement, D$state, sum_stats_vec)) |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        min     max         mean\nAK -4.10449 4.27101  0.003082000\nAL -3.63502 4.53867 -0.007811050\nAR -3.81380 4.10111  0.008453876\nAZ -4.41505 3.96512  0.000228734\nCA -4.15973 4.10245  0.013649303\nCO -3.86049 4.23142 -0.001334096\n```\n\n\n:::\n:::\n\n\nAt this point, I was also curious if the `do.call(\"rbind\",list)` can be sped up, so I constructed a second tapply solution.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsapply(tapply(D$measurement, D$state, sum_stats_vec), rbind) |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            AK          AL          AR           AZ         CA         CO\n[1,] -4.104494 -3.63502497 -3.81380048 -4.415050930 -4.1597256 -3.8604891\n[2,]  4.271009  4.53867192  4.10111494  3.965124829  4.1024464  4.2314151\n[3,]  0.003082 -0.00781105  0.00845388  0.000228734  0.0136493 -0.0013341\n              CT          DE          FL          GA          HI         IA\n[1,] -3.91246424 -4.13994780 -3.74126137 -3.94660998 -4.01231812 -3.7908228\n[2,]  4.42439303  4.10320343  3.61461913  3.92890675  3.54210746  4.1666241\n[3,]  0.00365691  0.00300726  0.00364475  0.00998416  0.00831473  0.0121738\n              ID          IL         IN         KS          KY          LA\n[1,] -3.96453912 -3.89433528 -4.0396596 -4.0290851 -4.04857519 -3.83912209\n[2,]  3.57787653  3.79539851  3.8997433  3.6210916  3.86743373  3.56924325\n[3,]  0.00364403  0.00282924 -0.0116455  0.0147175 -0.00141356  0.00150667\n              MA         MD         ME          MI          MN           MO\n[1,] -4.03713867 -3.7447056 -3.9230949 -3.95286252 -4.41058377 -3.939103495\n[2,]  3.79263060  3.7162898  3.6639929  4.15578354  4.31107098  4.428823170\n[3,] -0.00695907 -0.0104974  0.0017803  0.00578387  0.00222535 -0.000530262\n              MS         MT          NC         ND         NE          NH\n[1,] -4.11033773 -4.0871006 -4.37736785 -4.0689381 -4.0600632 -3.91574348\n[2,]  4.04043436  4.1620591  3.98113456  3.7439708  4.1786719  4.12714205\n[3,]  0.00401617 -0.0051991 -0.00791303 -0.0065667 -0.0019952  0.00173286\n              NJ         NM         NV          NY          OH          OK\n[1,] -3.89749099 -3.8077381 -4.4999793 -4.10688778 -3.97073238 -4.01749904\n[2,]  4.09297430  3.8533329  3.9841584  3.77539030  4.05541378  3.92196743\n[3,] -0.00821633 -0.0045123  0.0059095 -0.00401249  0.00391791  0.00272036\n               OR          PA          RI         SC          SD          TN\n[1,] -3.755405173 -3.87864920 -3.65614672 -3.6360017 -4.25212184 -3.63011318\n[2,]  4.299120255  4.18986336  4.25751403  4.1131445  3.74296173  3.92052537\n[3,]  0.000427857 -0.00303136 -0.00419174 -0.0110226  0.00774345  0.00671216\n              TX          UT         VA           VT          WA          WI\n[1,] -4.20588875 -4.03481832 -4.2980081 -3.749369728 -3.76023936 -3.76302646\n[2,]  3.92935839  3.99530205  3.8614655  3.990961895  3.87463693  4.32264119\n[3,] -0.00784648 -0.00561814 -0.0124286 -0.000579563  0.00684453  0.00297232\n             WV          WY\n[1,] -3.6034317 -3.86099776\n[2,]  3.9029500  4.11653276\n[3,] -0.0090395 -0.00238027\n```\n\n\n:::\n:::\n\n\nand we should obviously also include our new found `array2DF`\n\n\n::: {.cell}\n\n```{.r .cell-code}\narray2DF(tapply(D$measurement, D$state, sum_stats_vec)) |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Var1      min     max         mean\n1   AK -4.10449 4.27101  0.003082000\n2   AL -3.63502 4.53867 -0.007811050\n3   AR -3.81380 4.10111  0.008453876\n4   AZ -4.41505 3.96512  0.000228734\n5   CA -4.15973 4.10245  0.013649303\n6   CO -3.86049 4.23142 -0.001334096\n```\n\n\n:::\n:::\n\n\n## The obscure: reduce\n\nI thought that this should be it, but then I remembered `reduce` exists.\nThe solution is somewhat similar to split/lapply.\n\n::: {.cell}\n\n```{.r .cell-code}\nreduce <- function(D) {\n    state_list <- split(D$measurement, D$state)\n    Reduce(function(x, y) {\n        res <- sum_stats_vec(state_list[[y]])\n        rbind(x, data.frame(state = y, mean = res[1], min = res[2], max = res[3]))\n    }, names(state_list), init = NULL)\n}\n\nreduce(D) |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     state     mean     min          max\nmin     AK -4.10449 4.27101  0.003082000\nmin1    AL -3.63502 4.53867 -0.007811050\nmin2    AR -3.81380 4.10111  0.008453876\nmin3    AZ -4.41505 3.96512  0.000228734\nmin4    CA -4.15973 4.10245  0.013649303\nmin5    CO -3.86049 4.23142 -0.001334096\n```\n\n\n:::\n:::\n\n\n## The unfair contender: Rfast\n\nPondering about how this functions could be sped up in general, I remembered the package [`Rfast`](https://github.com/RfastOfficial/Rfast/) and managed to construct a solution using this package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nRfast <- function(D) {\n    lev_int <- as.numeric(D$state)\n    minmax <- Rfast::group(D$measurement, lev_int, method = \"min.max\")\n    data.frame(\n        state = levels(D$state),\n        mean = Rfast::group(D$measurement, lev_int, method = \"mean\"),\n        min = minmax[1, ],\n        max = minmax[2, ]\n    )\n}\n\nRfast(D) |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  state         mean      min     max\n1    AK  0.003082000 -4.10449 4.27101\n2    AL -0.007811050 -3.63502 4.53867\n3    AR  0.008453876 -3.81380 4.10111\n4    AZ  0.000228734 -4.41505 3.96512\n5    CA  0.013649303 -4.15973 4.10245\n6    CO -0.001334096 -3.86049 4.23142\n```\n\n\n:::\n:::\n\n\nPretty sure that this will be the fastest, maybe even competitive with the other big packages!\n\n## Benchmark\n\nFor better readability I reorder the benchmark results from `microbenchmark` according to median runtime, with a function provided by [Dirk Eddelbuettel](https://github.com/eddelbuettel/dang/blob/master/R/reorderMicrobenchmarkResults.R).\n\n::: {.cell}\n\n```{.r .cell-code}\nreorderMicrobenchmarkResults <- function(res, order = \"median\") {\n    stopifnot(\"Argument 'res' must be a 'microbenchmark' result\" = inherits(res, \"microbenchmark\"))\n\n    smry <- summary(res)\n    res$expr <- factor(res$expr,\n        levels = levels(res$expr)[order(smry[[\"median\"]])],\n        ordered = TRUE\n    )\n    res\n}\n```\n:::\n\n\nFirst up the \"small\" dataset with 1e6 rows. I added the `dplyr` and `data.table` results as references.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum_stats_list <- function(x) list(min = min(x), max = max(x), mean = mean(x))\nsum_stats_tibble <- function(x) tibble::tibble(min = min(x), max = max(x), mean = mean(x))\n\nbench1e6 <- microbenchmark::microbenchmark(\n    aggregate = aggregate(measurement ~ state, data = D, FUN = sum_stats_vec),\n    split_lapply = split_lapply(D),\n    array2DF_by = array2DF(by(D$measurement, D$state, sum_stats_vec)),\n    raw_by = by(D$measurement, D$state, sum_stats_vec),\n    docall_tapply = do.call(\"rbind\", tapply(D$measurement, D$state, sum_stats_vec)),\n    sapply_tapply = sapply(tapply(D$measurement, D$state, sum_stats_vec), rbind),\n    array2DF_tapply = array2DF(tapply(D$measurement, D$state, sum_stats_vec)),\n    reduce = reduce(D),\n    Rfast = Rfast(D),\n    dplyr = D |> dplyr::group_by(state) |> dplyr::summarise(sum_stats_tibble(measurement)) |> dplyr::ungroup(),\n    datatable = D[, .(sum_stats_list(measurement)), by = state],\n    times = 25\n)\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot2::autoplot(reorderMicrobenchmarkResults(bench1e6))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot-bench1e6-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|expr            |      min|       lq|     mean|   median|       uq|      max| neval|\n|:---------------|--------:|--------:|--------:|--------:|--------:|--------:|-----:|\n|datatable       |  13.4325|  15.2802|  16.8016|  15.5527|  17.6287|  22.9133|    25|\n|Rfast           |  13.5639|  16.5295|  19.1905|  18.0986|  21.3507|  33.5924|    25|\n|array2DF_tapply |  27.5491|  30.9502|  37.5948|  32.3145|  36.7538| 132.9784|    25|\n|docall_tapply   |  28.4628|  30.7052|  39.9436|  32.3174|  33.7132|  99.6504|    25|\n|sapply_tapply   |  28.8092|  30.1596|  46.0100|  33.3971|  37.7851| 101.5850|    25|\n|raw_by          |  40.9116|  44.1075|  52.2293|  45.9647|  48.4539| 119.5348|    25|\n|array2DF_by     |  43.2958|  45.8177|  58.4240|  48.8741|  52.2750| 132.8981|    25|\n|reduce          |  50.1459|  54.1688|  62.8492|  59.3776|  62.6560| 143.3252|    25|\n|dplyr           |  62.6194|  66.1931|  82.9065|  68.9263|  71.7941| 364.2103|    25|\n|split_lapply    |  84.7424|  90.0375| 110.7041|  96.4528| 113.7201| 168.3835|    25|\n|aggregate       | 319.9465| 335.0173| 386.0137| 369.7291| 429.6735| 538.2916|    25|\n\n\n:::\n:::\n\n\nFirst of, I was very surprised by the bad performance of `aggregate`. I looked at the source code and it appears to be a more fancy lapply/split type of functions with a lot of `if/else` and `for` which do slow down the function heavily. For the benchmark with the bigger dataset, I actually discarded the function because it was way too slow.\n\nApart from that, there are three groups. `Rfast` and `data.table` are the fastest clearly the fastest. The second group are the `tapply` versions. I am quite pleased with the fact that the data frame building via `do.call`, `sapply` and `array2DF` are very much comparable, because I really like my `array2DF` discovery. The remaining solutions are pretty much comparable. I am surprised though, that `dplyr` falls behind many of the base solutions.[^2]\n\nMoving on to the 100 million file to see if size makes a difference.\n\n::: {.cell}\n\n```{.r .cell-code}\nD <- data.table::fread(\"measurements1e8.csv\", stringsAsFactors = TRUE)\n\nbench1e8 <- microbenchmark::microbenchmark(\n    # aggregate = aggregate(measurement ~ state, data = D, FUN = sum_stats_vec),\n    split_lapply = split_lapply(D),\n    array2DF_by = array2DF(by(D$measurement, D$state, sum_stats_vec)),\n    raw_by = by(D$measurement, D$state, sum_stats_vec),\n    docall_tapply = do.call(\"rbind\", tapply(D$measurement, D$state, sum_stats_vec)),\n    sapply_tapply = sapply(tapply(D$measurement, D$state, sum_stats_vec), rbind),\n    array2DF_tapply = array2DF(tapply(D$measurement, D$state, sum_stats_vec)),\n    reduce = reduce(D),\n    Rfast = Rfast(D),\n    dplyr = D |> dplyr::group_by(state) |> dplyr::summarise(sum_stats_tibble(measurement)) |> dplyr::ungroup(),\n    datatable = D[, .(sum_stats_list(measurement)), by = state],\n    times = 10\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot2::autoplot(reorderMicrobenchmarkResults(bench1e8))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot-bench1e8-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|expr            |     min|      lq|    mean|  median|      uq|     max| neval|\n|:---------------|-------:|-------:|-------:|-------:|-------:|-------:|-----:|\n|Rfast           | 1.61404| 2.00889| 2.06342| 2.09445| 2.14546| 2.41314|    10|\n|datatable       | 2.17933| 2.20079| 2.29543| 2.23828| 2.33378| 2.70186|    10|\n|dplyr           | 2.80742| 2.87777| 3.04344| 3.00719| 3.17387| 3.45470|    10|\n|reduce          | 2.72298| 2.98724| 3.19725| 3.12715| 3.46963| 3.77594|    10|\n|docall_tapply   | 2.82332| 2.91701| 3.22054| 3.25852| 3.32141| 3.73731|    10|\n|sapply_tapply   | 2.78456| 2.81968| 3.19675| 3.29218| 3.43617| 3.66894|    10|\n|array2DF_tapply | 3.11413| 3.17007| 3.37320| 3.30678| 3.56680| 3.70316|    10|\n|array2DF_by     | 5.01008| 5.06045| 5.48980| 5.42499| 5.82906| 6.01545|    10|\n|raw_by          | 4.78366| 5.33826| 5.46399| 5.56182| 5.67001| 6.04721|    10|\n|split_lapply    | 5.95249| 6.44981| 6.55665| 6.56029| 6.84631| 6.87408|    10|\n\n\n:::\n:::\n\n\nAgain we see three groups, but this time with clearer cut-offs. `Rfast` and `data.table` dominate and Rfast actually has a slight edge! The second group are `tapply`, `reduce` and `dplyr`. Surprisingly, `by` falls behind here, together with `split/lapply`.\n\n**Update**(2024-01-09)  \n\nI managed to run some of the functions on a 1e9 file.\n\n```r\nbench1e9 <- microbenchmark::microbenchmark(\n    docall_tapply = do.call(\"rbind\", tapply(D$measurement, D$state, sum_stats_vec)),\n    reduce = reduce(D),\n    Rfast = Rfast(D),\n    dplyr = D |> dplyr::group_by(state) |> dplyr::summarise(sum_stats_tibble(measurement)) |> dplyr::ungroup(),\n    datatable = D[, .(sum_stats_list(measurement)), by = state],\n    times = 5\n)\n```\n![](bench1e9.png)\n\nThe previously fastest base solutions fall of a little bit, but are in my opinion still very good and still comparable with `dplyr`! Also, I learned that one can reorder microbenchmark results with the print command!\n\n## Summary\n\nThis was a fun little exercise, and I think I learned a lot of new things about base R, especially the existence of `arry2DF`! \n\nWhat was surprising is how competitive base R actually is with the \"big guns\". I was expecting a much bigger margin between data.table and the base solutions, but that was not the case. \n\n[^1]: Also inspired by a post of [Danielle Navarro](https://blog.djnavarro.net/posts/2023-12-27_seedcatcher/) about the cultural loss of today's serious blogging business.\n\n[^2]: It is far more readable though.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}