{
  "hash": "e6e2be220ad0acda0499e449b6272724",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"One billion row challenge using base R\"\nauthor:\n  - name: David Schoch\n    orcid: 0000-0003-2952-4812\ndate: 2024-01-08\ncategories: [R]\n---\n\n\nOne of my new years resolutions is to blog a bit more on the random shenanigans I do with R. This is one of those.\n\nThe [One Billion Row challenge](https://www.morling.dev/blog/one-billion-row-challenge/) by Gunnar Morling is as follows:\n\n> write a Java program for retrieving temperature measurement values from a text file and calculating the min, mean, and max temperature per weather station. Thereâ€™s just one caveat: the file has 1,000,000,000 rows!\n\nI didn't take long, also thanks to [Hacker News](https://news.ycombinator.com/item?id=38851337), that the challenge spread to other programming languages. The original repository contains a [show & tell](https://github.com/gunnarmorling/1brc/discussions/categories/show-and-tell) where other results can be discussed.\n\nObviously it also spread to R and there is a [GitHub repository](https://github.com/alejandrohagan/1br) from Alejandro Hagan dedicated to the challenge. There were some [critical discussions](https://github.com/alejandrohagan/1br/issues/5) on the seemingly bad performance of `data.table` but that issue thread also evolved to a discussion on other solutions. \n\nThe obvious candidates for fast solutions with R are [`dplyr`](https://github.com/tidyverse/dplyr), [`data.table`](https://github.com/Rdatatable/data.table), [`collapse`](https://github.com/SebKrantz/collapse), and [`polars`](https://github.com/pola-rs/r-polars). From those, it appears that polars might solve the tasks the [fastest](https://github.com/alejandrohagan/1br/issues/5#issuecomment-1879737918).\n\nI was curious, how far one can get with base R. \n\n## Creating the data\n\nThe R repository contains a [script](https://github.com/alejandrohagan/1br/blob/main/generate_data.R) to generate benchmark data. For the purpose of this post, I created files with 1e6 and 1e8 rows. Unfortunately, my personal laptop cannot handle 1 billion rows without dying.\n\n## Reading the data\n\nAll base R functions will profit from reading the station column as a factor instead of a usual string. How would have thought that `stringAsFactors = TRUE` can be useful.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nD <- data.table::fread(\"measurements1e6.csv\", stringsAsFactors = TRUE)\nD\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         measurement state\n      1:    0.981969    NC\n      2:    0.468715    MA\n      3:   -0.107971    TX\n      4:   -0.212878    VT\n      5:    1.158098    OR\n     ---                  \n 999996:    0.743249    FL\n 999997:   -1.685561    KS\n 999998:   -0.118455    TX\n 999999:    1.277437    MS\n1000000:   -0.280085    MD\n```\n\n\n:::\n:::\n\n\n## The obvious: aggregate and split/lapply\n\nThe most obvious choice for me was to use `aggregate()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum_stats_vec <- function(x) c(min = min(x), max = max(x), mean = mean(x))\naggregate(measurement ~ state, data = D, FUN = sum_stats_vec) |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  state measurement.min measurement.max measurement.mean\n1    AK    -4.104494064     4.271009490      0.003082000\n2    AL    -3.635024969     4.538671919     -0.007811050\n3    AR    -3.813800485     4.101114941      0.008453876\n4    AZ    -4.415050930     3.965124829      0.000228734\n5    CA    -4.159725627     4.102446367      0.013649303\n6    CO    -3.860489118     4.231415117     -0.001334096\n```\n\n\n:::\n:::\n\n\nI was pretty sure that this might be the best solution.\n\nThe other obvious solution is to split the data frame according to staty and then `lapply` the stats calculation on each list entry\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsplit_lapply <- function(D) {\n    result <- lapply(split(D, D$state), function(x) {\n        stats <- sum_stats_vec(x$measurement)\n        data.frame(\n            state = unique(x$state),\n            min = stats[1],\n            max = stats[2],\n            mean = stats[3]\n        )\n    })\n    do.call(\"rbind\", result)\n}\nsplit_lapply(D) |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   state      min     max         mean\nAK    AK -4.10449 4.27101  0.003082000\nAL    AL -3.63502 4.53867 -0.007811050\nAR    AR -3.81380 4.10111  0.008453876\nAZ    AZ -4.41505 3.96512  0.000228734\nCA    CA -4.15973 4.10245  0.013649303\nCO    CO -3.86049 4.23142 -0.001334096\n```\n\n\n:::\n:::\n\n\n## The elegant: by\n\nI stumbled upon `by` when searching for alternatives. I think it is a quite elegant looking way of solving a group/summarize task with base R. Unfortunately it returns a list and not a data frame or matrix (I made that an implicit requirement).  \n\nIn the help for `by` I stumbled upon a function I wasn't aware of yet: `array2DF`! \n\n::: {.cell}\n\n```{.r .cell-code}\narray2DF(by(D$measurement, D$state, sum_stats_vec)) |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  D$state      min     max         mean\n1      AK -4.10449 4.27101  0.003082000\n2      AL -3.63502 4.53867 -0.007811050\n3      AR -3.81380 4.10111  0.008453876\n4      AZ -4.41505 3.96512  0.000228734\n5      CA -4.15973 4.10245  0.013649303\n6      CO -3.86049 4.23142 -0.001334096\n```\n\n\n:::\n:::\n\n\nDoes exactly what is needed here. For the benchmarks, I will also include a version without the `array2DF` call, to check its overhead.\n\n## Another apply: tapply\n\nIn the help for `by`, I also stumbled upon this sentence \n\n> Function `by` is an object-oriented wrapper for `tapply` applied to data frames.\n\nSo maybe we can construct a solution that uses tapply, but without any inbuilt overhead in `by`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndo.call(\"rbind\", tapply(D$measurement, D$state, sum_stats_vec)) |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        min     max         mean\nAK -4.10449 4.27101  0.003082000\nAL -3.63502 4.53867 -0.007811050\nAR -3.81380 4.10111  0.008453876\nAZ -4.41505 3.96512  0.000228734\nCA -4.15973 4.10245  0.013649303\nCO -3.86049 4.23142 -0.001334096\n```\n\n\n:::\n:::\n\n\nAt this point, I was also curious if the `do.call(\"rbind\",list)` can be sped up, so I constructed a second tapply solution.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsapply(tapply(D$measurement, D$state, sum_stats_vec), rbind) |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            AK          AL          AR           AZ         CA         CO\n[1,] -4.104494 -3.63502497 -3.81380048 -4.415050930 -4.1597256 -3.8604891\n[2,]  4.271009  4.53867192  4.10111494  3.965124829  4.1024464  4.2314151\n[3,]  0.003082 -0.00781105  0.00845388  0.000228734  0.0136493 -0.0013341\n              CT          DE          FL          GA          HI         IA\n[1,] -3.91246424 -4.13994780 -3.74126137 -3.94660998 -4.01231812 -3.7908228\n[2,]  4.42439303  4.10320343  3.61461913  3.92890675  3.54210746  4.1666241\n[3,]  0.00365691  0.00300726  0.00364475  0.00998416  0.00831473  0.0121738\n              ID          IL         IN         KS          KY          LA\n[1,] -3.96453912 -3.89433528 -4.0396596 -4.0290851 -4.04857519 -3.83912209\n[2,]  3.57787653  3.79539851  3.8997433  3.6210916  3.86743373  3.56924325\n[3,]  0.00364403  0.00282924 -0.0116455  0.0147175 -0.00141356  0.00150667\n              MA         MD         ME          MI          MN           MO\n[1,] -4.03713867 -3.7447056 -3.9230949 -3.95286252 -4.41058377 -3.939103495\n[2,]  3.79263060  3.7162898  3.6639929  4.15578354  4.31107098  4.428823170\n[3,] -0.00695907 -0.0104974  0.0017803  0.00578387  0.00222535 -0.000530262\n              MS         MT          NC         ND         NE          NH\n[1,] -4.11033773 -4.0871006 -4.37736785 -4.0689381 -4.0600632 -3.91574348\n[2,]  4.04043436  4.1620591  3.98113456  3.7439708  4.1786719  4.12714205\n[3,]  0.00401617 -0.0051991 -0.00791303 -0.0065667 -0.0019952  0.00173286\n              NJ         NM         NV          NY          OH          OK\n[1,] -3.89749099 -3.8077381 -4.4999793 -4.10688778 -3.97073238 -4.01749904\n[2,]  4.09297430  3.8533329  3.9841584  3.77539030  4.05541378  3.92196743\n[3,] -0.00821633 -0.0045123  0.0059095 -0.00401249  0.00391791  0.00272036\n               OR          PA          RI         SC          SD          TN\n[1,] -3.755405173 -3.87864920 -3.65614672 -3.6360017 -4.25212184 -3.63011318\n[2,]  4.299120255  4.18986336  4.25751403  4.1131445  3.74296173  3.92052537\n[3,]  0.000427857 -0.00303136 -0.00419174 -0.0110226  0.00774345  0.00671216\n              TX          UT         VA           VT          WA          WI\n[1,] -4.20588875 -4.03481832 -4.2980081 -3.749369728 -3.76023936 -3.76302646\n[2,]  3.92935839  3.99530205  3.8614655  3.990961895  3.87463693  4.32264119\n[3,] -0.00784648 -0.00561814 -0.0124286 -0.000579563  0.00684453  0.00297232\n             WV          WY\n[1,] -3.6034317 -3.86099776\n[2,]  3.9029500  4.11653276\n[3,] -0.0090395 -0.00238027\n```\n\n\n:::\n:::\n\n\n## The obscure: reduce\n\nI thought that this should be it, but the I remembered `reduce` exists.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreduce <- function(D) {\n    state_list <- split(D$measurement, D$state)\n    Reduce(function(x, y) {\n        res <- sum_stats_vec(state_list[[y]])\n        rbind(x, c(state = y, mean = res[1], min = res[2], max = res[3]))\n    }, names(state_list), init = NULL)\n}\n\nreduce(D) |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     state mean.min            min.max            max.mean              \n[1,] \"AK\"  \"-4.10449406430213\" \"4.27100948968198\" \"0.00308199952547776\" \n[2,] \"AL\"  \"-3.63502496888963\" \"4.53867191854759\" \"-0.007811050080764\"  \n[3,] \"AR\"  \"-3.81380048494054\" \"4.10111494081779\" \"0.00845387582127389\" \n[4,] \"AZ\"  \"-4.41505092978418\" \"3.96512482869299\" \"0.00022873429539994\" \n[5,] \"CA\"  \"-4.15972562667134\" \"4.102446367264\"   \"0.0136493031701716\"  \n[6,] \"CO\"  \"-3.86048911804503\" \"4.23141511667196\" \"-0.00133409642877198\"\n```\n\n\n:::\n:::\n\n\ntechnically, this is a bit wrong since it it converts the numerical values to characters. But I allow it for this experiment.\n\n## The unfair contender: Rfast\n\nPondering about how this functions could be sped up in general, I remembered the package [`Rfast`](https://github.com/RfastOfficial/Rfast/) and managed to construct a solution using this package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nRfast <- function(D) {\n    lev_int <- as.numeric(D$state)\n    minmax <- Rfast::group(D$measurement, lev_int, method = \"min.max\")\n    data.frame(\n        state = levels(D$state),\n        mean = Rfast::group(D$measurement, lev_int, method = \"mean\"),\n        min = minmax[1, ],\n        max = minmax[2, ]\n    )\n}\n\nRfast(D) |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  state         mean      min     max\n1    AK  0.003082000 -4.10449 4.27101\n2    AL -0.007811050 -3.63502 4.53867\n3    AR  0.008453876 -3.81380 4.10111\n4    AZ  0.000228734 -4.41505 3.96512\n5    CA  0.013649303 -4.15973 4.10245\n6    CO -0.001334096 -3.86049 4.23142\n```\n\n\n:::\n:::\n\n\nPretty sure that this will be the fastest, maybe even competitive with the other big packages!\n\n## Benchmark\n\nTo reorder the benchmark results from `microbenchmark`, we use a function provided by [Dirk Eddelbuettel](https://github.com/eddelbuettel/dang/blob/master/R/reorderMicrobenchmarkResults.R).\n\n::: {.cell}\n\n```{.r .cell-code}\nreorderMicrobenchmarkResults <- function(res, order = \"median\") {\n    stopifnot(\"Argument 'res' must be a 'microbenchmark' result\" = inherits(res, \"microbenchmark\"))\n\n    smry <- summary(res)\n    res$expr <- factor(res$expr,\n        levels = levels(res$expr)[order(smry[[\"median\"]])],\n        ordered = TRUE\n    )\n    res\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsum_stats_list <- function(x) list(min = min(x), max = max(x), mean = mean(x))\nsum_stats_tibble <- function(x) tibble::tibble(min = min(x), max = max(x), mean = mean(x))\n\nbench1e6 <- microbenchmark::microbenchmark(\n    aggregate = aggregate(measurement ~ state, data = D, FUN = sum_stats_vec),\n    split_lapply = split_lapply(D),\n    array2DF_by = array2DF(by(D$measurement, D$state, sum_stats_vec)),\n    raw_by = by(D$measurement, D$state, sum_stats_vec),\n    docall_tapply = do.call(\"rbind\", tapply(D$measurement, D$state, sum_stats_vec)),\n    sapply_tapply = sapply(tapply(D$measurement, D$state, sum_stats_vec), rbind),\n    reduce = reduce(D),\n    Rfast = Rfast(D),\n    dplyr = D |> dplyr::group_by(state) |> dplyr::summarise(sum_stats_tibble(measurement)) |> dplyr::ungroup(),\n    datatable = D[, .(sum_stats_list(measurement)), by = state],\n    times = 25\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot2::autoplot(bench1e6)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/fig-bench1e6-1.png){#fig-bench1e6 width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nreorderMicrobenchmarkResults(bench1e6)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nUnit: milliseconds\n          expr      min       lq     mean   median       uq      max neval\n         Rfast  15.3668  16.4697  19.9911  19.1440  23.0988  31.1832    25\n     datatable  13.9852  18.2076  19.9162  19.9274  22.5450  24.2734    25\n docall_tapply  27.3638  31.8460  38.6104  34.1444  37.1358 121.5133    25\n sapply_tapply  27.3518  31.7411  37.1064  34.8800  39.2065  78.6460    25\n        reduce  26.3539  32.6424  38.6189  37.1253  41.9516  71.1157    25\n        raw_by  41.3559  46.7009  61.5596  53.6102  64.4096 142.2096    25\n   array2DF_by  43.8315  46.0743  66.0381  58.4842  63.5111 138.4939    25\n         dplyr  63.9571  67.6611  96.6385  84.0973  91.6902 431.6980    25\n  split_lapply  83.2171  91.7253 108.1934 106.7698 121.4588 166.9132    25\n     aggregate 313.0662 376.1037 417.4963 403.6883 471.1393 524.2444    25\n```\n\n\n:::\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}