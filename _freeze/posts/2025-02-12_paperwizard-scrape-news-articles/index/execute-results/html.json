{
  "hash": "89bf10608bf5f9a1566e90d417cacb29",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Paperwizard: Scrape News Sites using readability.js\"\nauthor:\n- name: David Schoch\n  orcid: 0000-0003-2952-4812\ndate: '2025-02-12'\ncategories:\n- R\n- package\n---\n\n\n\nIn this blog post I want to introduce my last academically motivated R package [`paperwizard`](https://github.com/schochastics/paperwizard).\nThe package is designed to extract readable content (such as news\narticles) from webpages using [Readability.js](https://github.com/mozilla/readability). \nTo do so, the package leverages `Node.js` to parse webpages and identify the main content of an article, allowing\nyou to work with cleaner, structured content.\n\nThe package is supposed to be an addon for [paperboy](https://github.com/jbgruber/paperboy), which implements\ncustom scraper for many international news websites.\n\n## Installation\n\nYou can install the package from GitHub\n\n``` r\npak::pak(\"schochastics/paperwizard\")\n```\n\nor r-universe\n\n```r\ninstall.packages(\"paperwizard\", repos = c(\"https://schochastics.r-universe.dev\", \"https://cloud.r-project.org\"))\n```\n\n## Setup\n\nTo use `paperwizard`, you need to have Node.js installed. Download and install Node.js from the [official\nwebsite](https://nodejs.org/en/download/package-manager). The page offers\ninstructions for all major OS. After installing Node.js, you can confirm the\ninstallation by running the following command in your terminal.\n```bash\nnode -v\n```\nThis should return the version of Node.js installed.\n\nTo make sure that the package knows where the command `node` is found, set \n```r\noptions(paperwizard.node_path = \"/path/to/node\")\n```\nif it is not installed in a standard location.\n\nOnce Node.js is installed, you need to install the necessary libraries which are\nlinkedom, Readability.js, puppeteer and axios. There is a convenient wrapper available in the package.\n\n```r\npw_npm_install()\n```\n\n## Using the package\n\nYou can use it either by supplying a url\n\n```r\npw_deliver(url)\n```\n\nor a data.frame that was created by `paperboy::pb_collect()`\n```r\nx <- paperboy::pb_collect(list_or_urls)\npw_deliver(x)\n```\n\n## Example\n\nTo get more insights on the returned objects, let us get a recent article from [The Conversation](https://theconversation.com/).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nurl <- \"https://theconversation.com/generative-ai-online-platforms-and-compensation-for-content-the-need-for-a-new-framework-242847\"\narticle <- paperwizard::pw_deliver(url)\nstr(article)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntibble [1 × 9] (S3: tbl_df/tbl/data.frame)\n $ url         : chr \"https://theconversation.com/generative-ai-online-platforms-and-compensation-for-content-the-need-for-a-new-framework-242847\"\n $ expanded_url: chr \"https://theconversation.com/generative-ai-online-platforms-and-compensation-for-content-the-need-for-a-new-framework-242847\"\n $ domain      : chr \"theconversation.com\"\n $ status      : num 200\n $ datetime    : POSIXct[1:1], format: \"2025-02-10 16:14:29\"\n $ author      : chr \"Thomas Paris\"\n $ headline    : chr \"Generative AI, online platforms and compensation for content: the need for a new framework\"\n $ text        : chr \"The emergence of generative artificial intelligence has put the issue of compensation for content producers bac\"| __truncated__\n $ misc        :List of 1\n  ..$ :List of 10\n  .. ..$ title        : chr \"Generative AI, online platforms and compensation for content: the need for a new framework\"\n  .. ..$ byline       : chr \"Thomas Paris\"\n  .. ..$ dir          : chr \"\"\n  .. ..$ lang         : chr \"en-EUROPE\"\n  .. ..$ content      : chr \"<DIV class=\\\"page\\\" id=\\\"readability-page-1\\\"><div itemprop=\\\"articleBody\\\">\\n    <p><strong>The emergence of g\"| __truncated__\n  .. ..$ textContent  : chr \"\\n    The emergence of generative artificial intelligence has put the issue of compensation for content produce\"| __truncated__\n  .. ..$ length       : int 5708\n  .. ..$ excerpt      : chr \"How will content creators be compensated for material used by artificial intelligence? Disputes involving tech \"| __truncated__\n  .. ..$ siteName     : chr \"The Conversation\"\n  .. ..$ publishedTime: chr \"2025-02-10T16:14:29Z\"\n```\n\n\n:::\n:::\n\n\n\nMost fields should be self explanatory. The `misc` field is a dump of the raw return values of the scraper for debugging or to get additional information that is not available in the standard fields.\n\n## Paperboy vs. Paperwizard\n\nAs I said in the introduction, `paperwizard` is meant to be an addon to `paperboy`.\nGenerally, it is always better to have a dedicated scraper for a news site, but building and\nmaintaining such a scraper, let alone dozens of them, is a lot of work. `Paperwizard` can help in situations where a\ndedicated scraper is either not available or currently broken. But given its generality, it does not mean that it will work for any\ngiven site without issues. It is always a good idea to at least check a few examples manually to verify that the scraper worked.\n\n## Important Considerations\n\nWhile web scraping is a valuable tool for data collection, it’s essential for researchers to approach it responsibly. \nResponsible web scraping helps ensure that data is collected ethically, legally, and in ways that protect both the integrity of the website and the privacy of individuals whose data may be included. If you are new to the topic, you can finde some help in this [GESIS DBD Guide](https://www.gesis.org/fileadmin/admin/Dateikatalog/pdf/guides/10_soldner_how_to_static_web_scraping.pdf). \n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}