{
  "hash": "dcfeda60c5b85ada6f8d1aa5377f0f56",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Be kind don't rbind\"\nauthor:\n  - name: David Schoch\n    orcid: 0000-0003-2952-4812\ndate: 2024-02-13\ncategories: [R]\n---\n\n\nThe other day I was helping to refactor an R package and came across one\nof the biggest performance blockers there is: dynamically growing matrices.\nOf course I repeated the mantra \"Always preallocate your variables\" but in this case,\nit is not clear how big (in terms of rows) the final matrix will be. So there is no\nway around growing the matrix dynamically. \n\nThe chosen approach in the package was to use `rbind` similar to this\n\n```r\nmat <- vector()\nwhile(<condition>) {\n    if(<condition>) {\n        tmp <- <do calculation>\n        mat <- rbind(mat, tmp)\n    }\n}\n```\n\nDisregarding performance, this seems like a sensible approach. Just add new rows to the end of the matrix. But a little bit of profiling showed that this was an extreme bottleneck in the function it appears. So what are viable alternatives? let's benchmark some solutions. \n\nAs a baseline we implement a function that preallocates memory. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nfmat <- function(n) {\n    res <- matrix(NA, n, n)\n    for (i in 1:n) {\n        res[i, ] <- runif(n)\n    }\n    res\n}\n```\n:::\n\n\nThe first contender is the `rbind` approach.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfrbind <- function(n) {\n    res <- vector()\n    for (i in 1:n) {\n        res <- rbind(res, runif(n))\n    }\n    res\n}\n```\n:::\n\n\nFor the second approach, we try to reduce the number of rbinds by growing the final matrix in chunks. For `csize = 1` we obtain `frbind()` and `csize = n` we have `fmat()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfchunks <- function(n, csize = 10) {\n    chunk <- matrix(NA, csize, n)\n    res <- vector()\n    for (i in 1:n) {\n        if (i %% csize == 0) {\n            chunk[csize, ] <- runif(n)\n            res <- rbind(res, chunk)\n            chunk <- matrix(NA, csize, n)\n        } else {\n            chunk[i %% csize, ] <- runif(n)\n        }\n    }\n    res[!is.na(res[, 1]), ]\n}\n```\n:::\n\n\nThe last approach is to grow list which is converted to a matrix at the end.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflist <- function(n) {\n    res <- list()\n    for (i in 1:n) {\n        res[[length(res) + 1]] <- runif(n)\n    }\n    do.call(rbind, res)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 1000\nbench <- microbenchmark::microbenchmark(\n    fmat(n),\n    frbind(n),\n    fchunks(n, csize = 10),\n    flist(n),\n    times = 1, unit = \"ms\"\n)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|expr                   |       min|        lq|      mean|    median|        uq|       max| neval|\n|:----------------------|---------:|---------:|---------:|---------:|---------:|---------:|-----:|\n|flist(n)               |   44.1759|   44.1759|   44.1759|   44.1759|   44.1759|   44.1759|     1|\n|fmat(n)                |   51.0986|   51.0986|   51.0986|   51.0986|   51.0986|   51.0986|     1|\n|fchunks(n, csize = 10) |  698.6495|  698.6495|  698.6495|  698.6495|  698.6495|  698.6495|     1|\n|frbind(n)              | 6012.3603| 6012.3603| 6012.3603| 6012.3603| 6012.3603| 6012.3603|     1|\n\n\n:::\n:::\n\n\nThe performance of the `rbind` approach is really terrifyingly bad. The list approach on the other hand is extremely efficient. It performs equally well as the preallocated matrix approach. I unfortunately lack the understanding of the R internals here, but it seems as if dynamically growing a list does not have any (or at least not much) overhead.",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}